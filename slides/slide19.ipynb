{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Maximum Entropy Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pods\n",
    "import mlai\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weather Forecast\n",
    "\n",
    "How do we forecast tomorrow's weather?\n",
    "\n",
    "- satellite\n",
    "- history, or one's experience\n",
    "- shoe\n",
    "\n",
    "How do we handle information from various sources?\n",
    "\n",
    "- **backoff:** choose one\n",
    "    1. use satellite image by default\n",
    "    2. rely on statistics (ie, past history) if 1. is not available\n",
    "    3. try a shoe prediction if 2. is not available\n",
    "\n",
    "- **interpolation:** weight each evidence, eg, $0.6\\times\\text{satellite} + 0.3\\times\\text{history} + 0.1\\times\\text{shoe}$\n",
    "\n",
    "- **maximum entropy:** make a least biased decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy\n",
    "\n",
    "**Coin tossing**\n",
    "\n",
    "Suppose a weighted coin has probability $h$ of coming up heads,\tentropy of tossing the coin only once is given by\n",
    "$$\n",
    "    H(X) = h \\log_2 \\frac{1}{h} + (1 - h) \\log_2 \\frac{1}{1 - h}\n",
    "$$\n",
    "\n",
    "Entropy is maximised when the coin is fair (\\ie, unbiased):\n",
    "<img src=\"./figs/coin_toss.jpg\", width=300, align=center>\n",
    "\n",
    "**Concept**\n",
    "\n",
    "- model *all* that is known\n",
    "- assume *nothing* that is unknown\n",
    "\n",
    "**Principle**\n",
    "\n",
    "- given a collection of facts, the **maximum entropy** method choose a model that is *consistent with all facts*, but otherwise *as uniform as possible*\n",
    "\n",
    "**Simple example**\n",
    "\n",
    "We wish to estimate a joint probability distribution $p(x, y)$ where $x\\in\\{x_1, x_2\\}$ and $y\\in\\{y_1, y_2\\}$, given the constraints\n",
    "\\begin{align*}\n",
    "    p(x_1, y_1) + p(x_2, y_1) &= 0.6 \\\\\n",
    "    p(x_1, y_1) + p(x_1, y_2) + p(x_2, y_1) + p(x_2, y_2) &= 1\n",
    "\\end{align*}\n",
    "Given these constraints, our objective is to maximise\n",
    "$$\n",
    "    H(X, Y) = \\sum_{x\\in\\{x_1, x_2\\}} \\sum_{y\\in\\{y_1, y_2\\}} p(x, y) \\log\\frac{1}{p(x, y)}\n",
    "$$\n",
    "\n",
    "One distribution that satisfies constrains:\n",
    "\n",
    ".     | $y_1$ | $y_2$ |\n",
    "------|-------|-------|--------\n",
    "$x_1$ | 0.5   | 0.1   |\n",
    "$x_2$ | 0.1   | 0.3   |\n",
    "total | 0.6   |       | 1.0\n",
    "\n",
    "The most uniform distribution that satisfies constrains:\n",
    "\n",
    ".     | $y_1$ | $y_2$ |\n",
    "------|-------|-------|--------\n",
    "$x_1$ | 0.3   | 0.2   |\n",
    "$x_2$ | 0.3   | 0.2   |\n",
    "total | 0.6   |       | 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### British Weather Forecast\n",
    "\n",
    "**Problem**\n",
    "\n",
    "Firstly, we make overly simplified assumption that five weather types, \\{misty, foggy, cloudy, sunny, rainy\\}, can fully describe British weather. Now, suppose that today's weather is \\{cloudy\\}, what will be tomorrow's weather?\n",
    "\n",
    "**A single constraint**\n",
    "\n",
    "Initially we have the *total probability* constraint:\n",
    "$$\n",
    "    p(misty) + p(foggy) + p(cloudy) + p(sunny) + p(rainy) = 1\n",
    "$$\n",
    "There exist infinite combinations of probabilities that may satisfy the constraint above.\n",
    "\n",
    "The most intuitively appealing model is\n",
    "\\begin{align*}\n",
    "    p(misty) &= 0.2 \\\\\n",
    "    p(foggy) &= 0.2 \\\\\n",
    "    p(cloudy) &= 0.2 \\\\\n",
    "    p(sunny) &= 0.2 \\\\\n",
    "    p(rainy) &= 0.2\n",
    "\\end{align*}\n",
    "This model allocates the total probability (that is ` 1 ') evenly among five possible weathers. It is the most *uniform* model subject to our knowledge... but what is exactly meant by *uniform*?\n",
    "\n",
    "Analytically, we wish to maximise the entropy $H(Y)$ given the total probability constraint. Using a Lagrange multiplier $\\lambda_1$:\n",
    "$$\n",
    "    \\Lambda = H(Y) + \\lambda_1 \\times \\text{constraint}\n",
    "    = \\left( - \\sum_{y\\in {\\cal Y}} p(y) \\log p(y) \\right) + \\lambda_1 \\left( \\sum_{y\\in {\\cal Y}} p(y) - 1 \\right)\n",
    "$$\n",
    "and take a partial derivative with respect to $p(y)$:\n",
    "$$\n",
    "    \\frac{\\partial\\Lambda}{\\partial p(y)} = - \\log p(y) - 1 + \\lambda_1\n",
    "$$\n",
    "Then set $\\displaystyle \\frac{\\partial\\Lambda}{\\partial p(y)} = 0$ and, finally, get $p(y) = 0.2$ for all $y$.\n",
    "\n",
    "**Two constraints**\n",
    "\n",
    "Suppose we have *two* constraints:\n",
    "\\begin{align*}\n",
    "    p(misty) + p(foggy) + p(cloudy) + p(sunny) + p(rainy) &= 1 \\\\\n",
    "    p(misty) + p(foggy) &= 0.3\n",
    "\\end{align*}\n",
    "By observation, the most *uniform* model is\n",
    "\\begin{align*}\n",
    "    p(misty) &= 0.15 \\\\\n",
    "    p(foggy) &= 0.15 \\\\\n",
    "    p(cloudy) &= 0.233... \\\\\n",
    "    p(sunny) &= 0.233... \\\\\n",
    "    p(rainy) &= 0.233...\n",
    "\\end{align*}\n",
    "\n",
    "Analytically, we maximise $H(Y)$ given those two constraints:\n",
    "\\begin{align*}\n",
    "    \\Lambda = H(Y)\n",
    "        & + \\lambda_1 \\{ p(misty) + p(foggy) + p(cloudy) + p(sunny) + p(rainy) - 1 \\} \\\\\n",
    "        & + \\lambda_2 \\{ p(misty) + p(foggy) - 0.3 \\}\n",
    "\\end{align*}\n",
    "and calculate partial derivatives:\n",
    "$$\n",
    "    \\frac{\\partial\\Lambda}{\\partial p(y)} = \\left\\{ \\begin{array}{ll}\n",
    "        - \\log p(y) - 1 + \\lambda_1 + \\lambda_2\t& y = misty, foggy \\\\\n",
    "        - \\log p(y) - 1 + \\lambda_1\t\t& \\text{otherwise}\n",
    "\t\\end{array} \\right.\n",
    "$$\n",
    "Then set $\\displaystyle \\frac{\\partial\\Lambda}{\\partial p(y)} = 0$.\n",
    "\n",
    "**Three constraints**\n",
    "\n",
    "Suppose we have *three* constraints:\n",
    "\\begin{align*}\n",
    "    p(misty) + p(foggy) + p(cloudy) + p(sunny) + p(rainy) &= 1 \\\\\n",
    "    p(misty) + p(foggy) &= 0.3 \\\\\n",
    "    p(misty) + p(cloudy) &= 0.5\n",
    "\\end{align*}\n",
    "\n",
    "Solution is no longer obvious, but we still can work on this case analytically.\n",
    "\n",
    "We maximise $H(Y)$ given three constraints:\n",
    "\\begin{align*}\n",
    "    \\Lambda = H(Y)\n",
    "        & + \\lambda_1 \\{ p(misty) + p(foggy) + p(cloudy) + p(sunny) + p(rainy) - 1 \\} \\\\\n",
    "        & + \\lambda_2 \\{ p(misty) + p(foggy) - 0.3 \\} \\\\\n",
    "        & + \\lambda_3 \\{ p(misty) + p(cloudy) - 0.5 \\}\n",
    "\\end{align*}\n",
    "Thus,\n",
    "$$\n",
    "    \\frac{\\partial\\Lambda}{\\partial p(y)} = \\left\\{ \\begin{array}{ll}\n",
    "        - \\log p(y) - 1 + \\lambda_1 + \\lambda_2 + \\lambda_3 & y = misty \\\\\n",
    "        - \\log p(y) - 1 + \\lambda_1 + \\lambda_2\t\t& y = foggy \\\\\n",
    "        - \\log p(y) - 1 + \\lambda_1 + \\lambda_3\t\t& y = cloudy \\\\\n",
    "        - \\log p(y) - 1 + \\lambda_1\t\t\t& \\text{otherwise}\n",
    "\t\\end{array} \\right.\n",
    "$$\n",
    "We set $\\displaystyle \\frac{\\partial\\Lambda}{\\partial p(y)} = 0$ and get the most *uniform* model:\n",
    "\\begin{align*}\n",
    "    p(misty) &= \\frac{9 - \\sqrt{51}}{10} = 0.186... \\\\\n",
    "    p(foggy) &= \\frac{\\sqrt{51} - 6}{10} = 0.114... \\\\\n",
    "    p(cloudy) &= \\frac{\\sqrt{51} - 4}{10} = 0.314... \\\\\n",
    "    p(sunny) &= \\frac{11 - \\sqrt{51}}{20} = 0.193... \\\\\n",
    "    p(rainy) &= \\frac{11 - \\sqrt{51}}{20} = 0.193...\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy Model\n",
    "\n",
    "**Random process**\n",
    "\n",
    "Formally, we define a random process as follows:\n",
    "\\begin{align*}\n",
    "    x &: \\text{some information influencing the output}, \\ x\\in{\\cal X} \\\\\n",
    "    y &: \\text{output value}, \\ y\\in{\\cal Y}\n",
    "\\end{align*}\n",
    "\n",
    "- (eg) a random process defined for the British weather forecast problem\n",
    "\\begin{align*}\n",
    "    x &: \\text{today's weather}, \\ x\\in\\{cloudy\\} \\\\\n",
    "    y &: \\text{tomorrow's weather}, \\ y\\in\\{misty, foggy, cloudy, sunny, rainy\\}\n",
    "\\end{align*}\n",
    "\n",
    "**Training samples**\n",
    "\n",
    "We also have training samples $(x_1, y_1), (x_2, y_2), \\ \\ldots \\ , (x_N, y_N)$ .\n",
    "\n",
    "- (eg) ten training samples for the British weather forecast problem\n",
    "\\begin{align*}\n",
    "    & (cloudy, cloudy), (cloudy, sunny), (cloudy, sunny), (cloudy, misty), (cloudy, cloudy), \\\\\n",
    "    & (cloudy, rainy), (cloudy, misty), (cloudy, foggy), (cloudy, cloudy), (cloudy, rainy)\n",
    "\\end{align*}\n",
    "\n",
    "**Feature**\n",
    "\n",
    "For $i = 1,\\ldots,n$ ($n$: number of features), we define $f_i(x, y)$, an indicator function of type ${\\cal X}\\times{\\cal Y}\\longrightarrow\\{0, 1\\}$ .\n",
    "\n",
    "- (eg) a feature set for the British weather forecast problem\n",
    "\\begin{align*}\n",
    "    f_1(x, y) = 1 \\quad & \\text{if} \\ y = \\{misty, foggy, cloudy, sunny, rainy\\} \\\\\n",
    "    f_2(x, y) = 1 \\quad & \\text{if} \\ y = \\{misty, foggy\\} \\\\\n",
    "    f_3(x, y) = 1 \\quad & \\text{if} \\ y = \\{misty, cloudy\\}\n",
    "\\end{align*}\n",
    "otherwise $f_i(x, y) = 0$ for $i = 1,2,3$ .\n",
    "\n",
    "    - note that $x$ is always $\\{cloudy\\}$ with this problem\n",
    "    - if one sample is $(cloudy, foggy)$: $f_1 = 1, f_2 = 1, f_3 = 0$\n",
    "\n",
    "**Expected values**\n",
    "\n",
    "The expected value of $f_i$ with respect to an *empirical distribution* $\\tilde{p}(x, y)$ is given by\n",
    "$$\n",
    "    \\tilde{p}(f_i) \\equiv E_{\\tilde{p}}[f_i] = \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x, y) f_i(x, y)\n",
    "$$\n",
    "\n",
    "- $\\tilde{p}(x, y)$ represents a summary of the training sample, that is,\n",
    "  $\\displaystyle \\tilde{p}(x, y) \\equiv \\frac{1}{N} \\times \\text{number of times that $(x, y)$ occurs in the sample}$\n",
    "- some pair $(x, y)$ may not occur at all in the sample\n",
    "\n",
    "The expected value of $f_i$ with respect to a *model distribution* $p(x, y)$ is given by\n",
    "$$\n",
    "    p(f_i) \\equiv E_p[f_i] = \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} p(x, y) f_i(x, y)\n",
    "    \\sim \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\sum_{y\\in {\\cal Y}} p(y | x) f_i(x, y)\n",
    "$$\n",
    "\n",
    "- calculation of $p(f_i)$ with respect to $p(x, y)$ is to the order of $|\\ {\\cal X}\\times{\\cal Y}\\ |$, which is often too large\n",
    "- instead, by using the empirical distribution $\\tilde{p}(x)$, the calculation gets more tractable because we only consider those in the training sample\n",
    "- we are likely to have more reliable estimates for $p(y | x)$ than for $p(x, y)$\n",
    "\n",
    "Now we have the following **constraint** that relates two expected values:\n",
    "$$\n",
    "    p(f_i) = \\tilde{p}(f_i)\n",
    "$$\n",
    "where $\\tilde{p}(f_i)$ is a mean of representing statistical phenomena in the training sample, and $p(f_i) = \\tilde{p}(f_i)$ is a mean of requiring that our model generalises these phenomena.\n",
    "\n",
    "- (eg) constraints for the British weather forecast problem\n",
    "  \\begin{align*}\n",
    "    p(misty) + p(foggy) + p(cloudy) + p(sunny) + p(rainy) &= 1 \\\\\n",
    "    p(misty) + p(foggy) &= 0.3 \\\\\n",
    "    p(misty) + p(cloudy) &= 0.5\n",
    "  \\end{align*}\n",
    "\n",
    "**Maximum entropy model**\n",
    "\n",
    "Of all conditional probability distributions ${\\cal P}$, a subset ${\\cal C} \\equiv \\{ \\ p\\in{\\cal P} \\ | \\ p(f_i) = \\tilde{p}(f_i) \\ \\text{ for } \\ i = 1,\\ldots,n \\ \\}$ constrains the model according to our knowledge.\n",
    "The following conditional entropy indicates a *mathematical measure of the uniformity*:\n",
    "$$\n",
    "    H_p(Y|X) \\equiv - \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\sum_{y\\in {\\cal Y}} p(y | x) \\log p(y | x)\n",
    "$$\n",
    "where notation $H_p(Y|X)$ emphasises the dependency of the entropy on $p$.\n",
    "\n",
    "Finally, we have reached the maximum entropy model of the form:\n",
    "$$\n",
    "    p* = \\mathop{\\rm argmax}_{p\\in{\\cal C}} H_p(Y|X)\n",
    "$$\n",
    "\n",
    "**Parameter estimation procedure**\n",
    "\n",
    "For each feature $f_i \\ (i = 1,\\ldots,n)$, we introduce a *Lagrange multiplier* $\\lambda_i$, then define the *Lagrangian*:\n",
    "$$\n",
    "    \\Lambda(p, \\lambda) \\equiv H_p(Y|X) + \\sum_i \\lambda_i \\{ p(f_i) - \\tilde{p}(f_i) \\}\n",
    "$$\n",
    "    \n",
    "Holding $\\lambda = \\{\\lambda_i\\}$ fixed, we compute the unconstrained maximum of $\\Lambda(p, \\lambda)$ over all $p\\in{\\cal P}$ :\n",
    "\\begin{align*}\n",
    "    p_{\\lambda}(y|x) &= \\mathop{\\rm argmax}_{p\\in{\\cal P}} \\Lambda(p, \\lambda) \\\\\n",
    "\t\\Phi(\\lambda) &= \\Lambda(p_{\\lambda}, \\lambda)\n",
    "\\end{align*}\n",
    "\n",
    "General solution is the exponential model:\n",
    "\\begin{align*}\n",
    "    p_{\\lambda}(y|x) &= \\frac{1}{Z_{\\lambda}(x)} \\exp \\left( \\sum_i \\lambda_i f_i(x,y) \\right) \\\\\n",
    "    \\Phi(\\lambda) &= - \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\log Z_{\\lambda}(x) + \\sum_i \\lambda_i \\tilde{p}(f_i)\n",
    "\\end{align*}\n",
    "where $Z_{\\lambda}(x)$ is a normalising constant given by\n",
    "$$\n",
    "    Z_{\\lambda}(x) = \\sum_{y\\in {\\cal Y}} \\exp \\left( \\sum_i \\lambda_i f_i(x,y) \\right)\n",
    "$$\n",
    "Technically, $\\lambda_i$ is a *Lagrange multiplier*, associated with the feature $f_i(x,y)$, in a certain constrained optimisation problem. In a sense, $\\lambda_i$ is a measure of the *importance* of the feature $f_i(x,y)$ ."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
