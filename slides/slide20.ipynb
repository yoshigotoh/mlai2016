{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Iterative Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pods\n",
    "import mlai\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "**Auxiliary function**\n",
    "\n",
    "An auxiliary function is a (pointwise) lower or upper bound on a function.\n",
    "<img src=\"./figs/auxiliary.jpg\", width=300, align=center>\n",
    "\n",
    "Because $x - 1 \\geq \\log x$ (see the figure above), $x - 1$ is an auxiliary function for $\\log x$ (and $\\log x$ is an auxiliary function for $x - 1$).\n",
    "\n",
    "**Maximum likelihood model**\n",
    "\n",
    "Supposing that the symbol $y$ is observed $\\tilde{c}(y)$ times in a sample sequence of size $T$, the probability of symbol $y$ is given by $\\displaystyle \\tilde{p}(y) = \\frac{\\tilde{c}(y)}{T}$.\n",
    "\n",
    "The **likelihood** of the sample with respect to a model $p(y)$ is\n",
    "$$\n",
    "    {\\cal P}(\\tilde{p} | p) = \\prod_{y\\in {\\cal Y}} p(y)^{\\tilde{c}(y)}\n",
    "$$\n",
    "\n",
    "There is a monotone relation to the *average per symbol log likelihood*:\n",
    "$$\n",
    "    {\\cal L}(\\tilde{p} | p) = \\sum_{y\\in {\\cal Y}} \\tilde{p}(y) \\log p(y)\n",
    "$$\n",
    "\n",
    "And we define the **maximum likelihood model** using the log likelohood:\n",
    "$$\n",
    "\tp* = \\mathop{\\rm argmax}_p {\\cal L}(\\tilde{p} | p)\n",
    "$$\n",
    "\n",
    "We are interested in the gain in log likelihood, ${\\cal L}(\\tilde{p} | p_{\\theta'}) - {\\cal L}(\\tilde{p} | p_{\\theta})$, of using one model $p_{\\theta'}$ instead of another model $p_{\\theta}$ .\n",
    "\n",
    "We define an *auxiliary function* ${\\cal A}(\\theta' | \\theta)$ such that\n",
    "\\begin{align*}\n",
    "\t{\\cal L}(\\tilde{p} | p_{\\theta'}) - {\\cal L}(\\tilde{p} | p_{\\theta}) &\\geq {\\cal A}(\\theta' | \\theta) \\\\\n",
    "\t{\\cal A}(\\theta | \\theta) &= 0\n",
    "\\end{align*}\n",
    "    \n",
    "Suppose we have a $\\theta$ and, if we find a $\\theta'$ that satisfies ${\\cal A}(\\theta' | \\theta) > 0$, then $p_{\\theta'}$ is a better model than $p_{\\theta}$ (in a maximum likelihood sense).\n",
    "\n",
    "Let $\\displaystyle p_{\\theta}(y) = \\sum_{z\\in {\\cal Z}} p_{\\theta}(y, z)$, and we have\n",
    "\\begin{align*}\n",
    "    {\\cal L}(\\tilde{p} | p_{\\theta'}) - {\\cal L}(\\tilde{p} | p_{\\theta})\n",
    "\t&= \\sum_{y\\in {\\cal Y}} \\tilde{p}(y) \\log \\frac{p_{\\theta'}(y)}{p_{\\theta}(y)} \\\\\n",
    "\t&= \\sum_{y\\in {\\cal Y}} \\tilde{p}(y) \\log \\sum_{z\\in {\\cal Z}} \\frac{p_{\\theta'}(y, z) p_{\\theta}(z | y)} {p_{\\theta}(y, z)} \\\\\n",
    "\t&\\geq \\underbrace{\\sum_{y\\in {\\cal Y}} \\tilde{p}(y) \\sum_{z\\in {\\cal Z}} p_{\\theta}(z | y) \\log \\frac{p_{\\theta'}(y, z)} {p_{\\theta}(y, z)}}_{{\\cal A}(\\theta' | \\theta)}\n",
    "\\end{align*}\n",
    "\n",
    "**EM algorithm** starts with some initial value $\\theta^{[0]}$, then iterates\n",
    "- *E step*: compute ${\\cal A}(\\theta' | \\theta)$\n",
    "- *M step*: update $\\displaystyle \\theta \\leftarrow \\mathop{\\rm argmax}_{\\theta'}{\\cal A}(\\theta' | \\theta)$\n",
    "\n",
    "**Linear interpolation example**\n",
    "\n",
    "We have observed samples $y = \\{y_1, \\ldots, y_T\\}$ and a collection of models $p_1(y), \\ldots, p_N(y)$.\n",
    "We wish to find the maximum likelihood distribution of\n",
    "$$\n",
    "    p(y) = \\sum_{i = 1\\ldots N} c_i p_i(y)\n",
    "$$\n",
    "given a constraint $\\displaystyle \\sum_{i = 1\\ldots N} c_i = 1$ .\n",
    "This is an $(N - 1)$ dimensional search problem, which is easy by the **EM algorithm**.\n",
    "\n",
    "Here's how the dataset looks like:\n",
    "<img src=\"./figs/dataset.jpg\", width=500, align=center>\n",
    "\n",
    "We denote the probability of using the $i^{th}$ model $p_i(y_t)$ and producing output $y_t$ by\n",
    "$$\n",
    "    p_c(y_t, i) = c_i p_i(y_t)\n",
    "$$\n",
    "and the probability of using $p_i(y_t)$, suppose the current output is $y_t$, by\n",
    "$$\n",
    "    p_c(i | y_t) = \\frac{c_i p_i(y_t)} {\\displaystyle \\sum_{j = 1\\ldots N} c_j p_j(y_t)}\n",
    "$$\n",
    "\n",
    "Now the *auxiliary function* for $c = \\{c_1, \\ldots, c_N\\}$ is written as\n",
    "$$\n",
    "    {\\cal A}(c' | c) = \\sum_{t = 1\\ldots T} \\tilde{p}(y_t) \\sum_{i = 1\\ldots N} p_c(i | y_t) \\log\\frac{p_{c'}(y_t, i)}{p_c(y_t, i)}\n",
    "$$\n",
    "\n",
    "Using a *Lagrange multiplier* $\\gamma$,\n",
    "$$\n",
    "    0\n",
    "    = \\frac{\\partial}{\\partial c_i'} \\left\\{ {\\cal A}(c' | c) + \\gamma \\left( \\sum_{i = 1\\ldots N} c_i' - 1 \\right) \\right\\}\n",
    "    = \\frac{1}{c_i'} \\sum_{t = 1\\ldots T} \\tilde{p}(y_t) p_c(i | y_t) + \\gamma\n",
    "$$\n",
    "which results in $\\gamma = -1$, and hence we update $c_i$ using\n",
    "$$\n",
    "    c_i'\n",
    "    = \\sum_{t = 1\\ldots T} \\tilde{p}(y_t) p_c(i | y_t)\n",
    "    = \\sum_{t = 1\\ldots T} \\tilde{p}(y_t) \\frac{c_i p_i(y_t)} {\\displaystyle \\sum_{j = 1\\ldots N} c_j p_j(y_t)}\n",
    "$$\n",
    "\n",
    "Here's a mixture of two Normal distributions:\n",
    "<img src=\"./figs/mixture.jpg\", width=500, align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Numerical Solution\n",
    "\n",
    "We can formulate the task (ie, that of finding a solution to the maximum entropy model) as a pure *maximum likelihood* problem of discovering the optimal values for the model parameters.\n",
    "\n",
    "We start with the log likelihood of the general solution $p_{\\lambda}(y|x)$:\n",
    "\\begin{align*}\n",
    "    {\\cal L}(\\tilde{p} | p_{\\lambda}) &= \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) \\log p_{\\lambda}(y|x) \\\\\n",
    "    &= \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) \\sum_i \\lambda_i f_i(x,y) - \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\log Z_{\\lambda}(x) \\underbrace{\\sum_{y\\in {\\cal Y}} \\tilde{p}(y|x)}_1\n",
    "\\end{align*}\n",
    "\n",
    "Differentiating ${\\cal L}(\\tilde{p} | p_{\\lambda})$ with respect to an individual $\\lambda_i$,\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial {\\cal L}(\\tilde{p} | p_{\\lambda})}{\\partial \\lambda_i}\n",
    "    &= \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) \\frac{\\partial}{\\partial \\lambda_i} \\sum_i \\lambda_i f_i(x,y) - \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\frac{\\partial \\log Z_{\\lambda}(x)}{\\partial \\lambda_i} \\\\\n",
    "    &= \\underbrace{\\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) f_i(x,y)}_{\\tilde{p}(f_i)} - \\underbrace{\\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\sum_{y\\in {\\cal Y}} p_{\\lambda}(y|x) f_i(x,y)}_{p(f_i)}\n",
    "\\end{align*}\n",
    "\n",
    "Setting $\\displaystyle \\frac{\\partial {\\cal L}(\\tilde{p} | p_{\\lambda})}{\\partial \\lambda_i}$ to zero yields the condition for maximising the log likelihood with respect to $\\lambda_i$.\n",
    "In other words, the maximum entropy model $p*\\in{\\cal C}$ also maximises the *likelihood* of the training sample $\\tilde{p}$ .\n",
    "\n",
    "A **numerical solution** is the procedure that iteratively achieves\n",
    "$$\n",
    "    \\lambda \\equiv \\{ \\lambda_1, \\ldots, \\lambda_n \\}\n",
    "\t\\quad\\longrightarrow\\quad \\lambda + \\delta \\equiv\n",
    "\t\\{ \\lambda_1 + \\delta_1, \\ldots, \\lambda_n + \\delta_n \\}\n",
    "$$\n",
    "where $\\lambda + \\delta$ is not inferior to $\\lambda$ in a *log likelihood* sense.\n",
    "\n",
    "To achieve this, we calculate\n",
    "\\begin{align*}\n",
    "    {\\cal L}(\\tilde{p} | p_{\\lambda + \\delta}) - {\\cal L}(\\tilde{p} | p_{\\lambda})\n",
    "    &= \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) \\log p_{\\lambda + \\delta}(y|x) - \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) \\log p_{\\lambda}(y|x) \\\\\n",
    "    &= \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) \\sum_i \\delta_i f_i(x,y) - \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\log \\frac{Z_{\\lambda + \\delta}(x)}{Z_{\\lambda}(x)}\n",
    "\\end{align*}\n",
    "\n",
    "Using the inequality $-\\log a \\geq 1 - a$ for all $a > 0$, we establish a lower bound on the change in the log likelihood:\n",
    "$$\n",
    "    {\\cal L}(\\tilde{p} | p_{\\lambda + \\delta}) - {\\cal L}(\\tilde{p} | p_{\\lambda}) \\geq {\\cal A}(\\delta | \\lambda)\n",
    "$$\n",
    "where\n",
    "\\begin{align*}\n",
    "    {\\cal A}(\\delta | \\lambda)\n",
    "    = & \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) \\sum_i \\delta_i f_i(x,y) + 1 - \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\frac{Z_{\\lambda + \\delta}(x)}{Z_{\\lambda}(x)} \\\\\n",
    "    = & \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) \\sum_i \\delta_i f_i(x,y) + 1 - \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\sum_{y\\in {\\cal Y}} p_{\\lambda}(y|x) \\exp \\left( \\sum_i \\delta_i f_i(x,y) \\right)\n",
    "\\end{align*}\n",
    "If we find $\\delta$ for which \\ ${\\cal A}(\\delta | \\lambda) > 0$, then $\\lambda + \\delta$ is an improvement over $\\lambda$.\n",
    "\n",
    "**problem:**\n",
    "the straightforward maximisation of ${\\cal A}(\\delta | \\lambda)$ may not work, because differentiating ${\\cal A}(\\delta | \\lambda)$ with respect to $\\delta_i$ yields an equation containing $\\{ \\delta_1, \\ldots, \\delta_n \\}$ (ie, $\\delta_i$ is coupled).\n",
    "\n",
    "**solution:**\n",
    "to get around above problem, we introduce the quantity: $\\displaystyle f^{\\#}(x,y) \\equiv \\sum_i f_i(x,y)$, ie, the number of features that are not zero at $(x,y)$, and rewrite:\n",
    "$$\n",
    "    \\exp \\left( \\sum_i \\delta_i f_i(x,y) \\right) = \\exp \\left( \\sum_i \\frac{f_i(x,y)}{f^{\\#}(x,y)} \\delta_i f^{\\#}(x,y) \\right)\n",
    "$$\n",
    "\n",
    "Notice that $\\displaystyle \\frac{f_i(x,y)}{f^{\\#}(x,y)}$ is a pdf over $i$, that is non-negative and sums to one.\n",
    "\n",
    "For a convex function, $f(x) = e^x$, *Jensen's inequality* says\n",
    "\\begin{align*}\n",
    "    f(E[i]) &\\leq E[f(i)] \\\\\n",
    "    \\exp \\left( \\sum_i p(i) q(i) \\right) &\\leq \\sum_i p(i) \\exp q(i) \\\\\n",
    "    \\exp \\left( \\sum_i \\frac{f_i(x,y)}{f^{\\#}(x,y)} \\delta_i f^{\\#}(x,y)\\right) &\\leq \\sum_i \\frac{f_i(x,y)}{f^{\\#}(x,y)} \\exp \\left( \\delta_i f^{\\#}(x,y) \\right)\n",
    "\\end{align*}\n",
    "which results in ${\\cal A}(\\delta | \\lambda) \\geq {\\cal B}(\\delta | \\lambda)$ where\n",
    "$$\n",
    "    {\\cal B}(\\delta | \\lambda) = \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) \\sum_i \\delta_i f_i(x,y) + 1 - \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\sum_{y\\in {\\cal Y}} p_{\\lambda}(y|x) \\sum_i \\frac{f_i(x,y)}{f^{\\#}(x,y)} \\exp \\left( \\delta_i f^{\\#}(x,y) \\right)\n",
    "$$\n",
    "\n",
    "${\\cal B}(\\delta | \\lambda)$ is a new, not as tight, lower bound on the change in log likelihood:\n",
    "$$\n",
    "    {\\cal L}(\\tilde{p} | p_{\\lambda + \\delta}) - {\\cal L}(\\tilde{p} | p_{\\lambda}) \\geq {\\cal B}(\\delta | \\lambda)\n",
    "$$\n",
    "\n",
    "Differentiating ${\\cal B}(\\delta | \\lambda)$ with respect to $\\delta_i$,\n",
    "$$\n",
    "    \\frac{\\partial {\\cal B}(\\delta | \\lambda)}{\\partial \\delta_i} = \\overbrace{\\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} \\tilde{p}(x,y) f_i(x,y)}^{\\tilde{p}(f_i)} - \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\sum_{y\\in {\\cal Y}} p_{\\lambda}(y|x) f_i(x,y) \\exp \\left( \\delta_i f^{\\#}(x,y) \\right)\n",
    "$$\n",
    "\n",
    "What is nice about this? $\\quad\\Longrightarrow\\quad$ $\\delta_i$ appears alone without any coupling.\n",
    "\n",
    "**Iterative scaling algorithm (ISA)**\n",
    "\n",
    "1. initially set $\\lambda_i = 0$ for all $i = 1,\\ldots,n$\n",
    "\n",
    "2. do for each $i = 1,\\ldots,n$ :\n",
    "\n",
    "    A. let $\\delta_i$ be the solution to\n",
    "    $$\n",
    "        \\displaystyle \\sum_{x\\in {\\cal X}} \\tilde{p}(x) \\sum_{y\\in {\\cal Y}} p_{\\lambda}(y|x) f_i(x,y) \\exp\\left( \\delta_i f^{\\#}(x,y) \\right) = \\tilde{p}(f_i)\n",
    "    $$\n",
    "    where $\\displaystyle f^{\\#}(x,y) \\equiv \\sum_{i = 1,\\ldots,n} f_i(x,y)$\n",
    "\n",
    "\tB. update multipliers by $\\lambda_i \\leftarrow \\lambda_i + \\delta_i$\n",
    "\n",
    "3. go back to step 2, if not all $\\lambda_i$ have converged\n",
    "\n",
    "**British weather forecast problem (revisited)**\n",
    "\n",
    "The numbers of active features for \\{misty, foggy, cloudy, sunny, rainy\\} are 3, 2, 2, 1, and 1, respectively.\n",
    "\n",
    "We denote $a = e^{\\delta_1}$, $b = e^{\\delta_2}$ and $c = e^{\\delta_3}$, and define:\n",
    "\\begin{align*}\n",
    "    {\\cal G}(a) &= p(misty)\\cdot a^3 + p(foggy)\\cdot a^2 + p(cloudy)\\cdot a^2 + p(sunny)\\cdot a + p(rainy)\\cdot a - 1 \\\\\n",
    "\t{\\cal G}(b) &= p(misty)\\cdot b^3 + p(foggy)\\cdot b^2 - 0.3 \\\\\n",
    "\t{\\cal G}(c) &= p(misty)\\cdot c^3 + p(cloudy)\\cdot c^2 - 0.5\n",
    "\\end{align*}\n",
    "\n",
    "To get $a$, $b$, and $c$, we achieve the *Newton-Raphson*, ie, $\\displaystyle a_{n+1} = a_n - \\frac{{\\cal G} (a_n)}{{\\cal G}' (a_n)}$ .\n",
    "\n",
    "We are able to calculate probabilities from multipliers $\\lambda_1$, $\\lambda_2$, and $\\lambda_3$ :\n",
    "\\begin{align*}\n",
    "    p(misty) &= \\frac{1}{Z_{\\lambda}} \\exp (\\lambda_1 + \\lambda_2 + \\lambda_3) \\\\\n",
    "\tp(foggy) &= \\frac{1}{Z_{\\lambda}} \\exp (\\lambda_1 + \\lambda_2) \\\\\n",
    "\tp(cloudy) &= \\frac{1}{Z_{\\lambda}} \\exp (\\lambda_1 + \\lambda_3) \\\\\n",
    "    p(sunny) &= \\frac{1}{Z_{\\lambda}} \\exp \\lambda_1 \\\\\n",
    "\tp(rainy) &= \\frac{1}{Z_{\\lambda}} \\exp \\lambda_1\n",
    "\\end{align*}\n",
    "\n",
    "Here's the graph, produced by applying the *iterative scaling algorithm*:\n",
    "<img src=\"./figs/newton.jpg\", width=500, align=center>\n",
    "\n",
    "Comparison of numerical calculation (up to 20 iterations) and analytical calculation:\n",
    "  \n",
    ".             |  0    | 1     | $\\cdots$ | 20    | | analytical\n",
    "--------------|-------|-------|----------|-------|-|-----------\n",
    "$p($misty$)$  | 0.333 | 0.168 |          | 0.186 | | 0.186\n",
    "$p($foggy$)$  | 0.222 | 0.175 |          | 0.115 | | 0.114\n",
    "$p($cloudy$)$ | 0.222 | 0.213 |          | 0.312 | | 0.314\n",
    "$p($sunny$)$  | 0.111 | 0.222 |          | 0.193 | | 0.193\n",
    "$p($rainy$)$  | 0.111 | 0.222 |          | 0.193 | | 0.193\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
