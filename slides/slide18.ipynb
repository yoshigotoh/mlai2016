{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Uncertainty and Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pods\n",
    "import mlai\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Uncertainty\n",
    "\n",
    "Do you agree ---\n",
    "\n",
    "- a race between _three_ equally matched horses is **more uncertain** than a race between _two_ equally matched horses\n",
    "- the outcome of a spin on a _roulette wheel_ is **more uncertain** than the throw of a _die_\n",
    "- the throw of a _fair die_ is **more uncertain** than the throw of a _biased die_ (eg, showing 1 with probability 0.5 and each of the rest with 0.1)\n",
    "\n",
    "???\n",
    "\n",
    "**Observation:** same uncertainties for\n",
    "\n",
    "- a flip of a biased coin, with the head coming up as twice frequently as the tail\n",
    "- a flip of a biased coin, with the tail coming up as twice frequently as the head\n",
    "- a race between two horses, with one of horses winning as twice more likely as the other\n",
    "\n",
    "The uncertainty of a random variable $X$, that takes values $a_i$ with probabilities $p_i$ for $1\\leq i\\leq n$, should be a function only of the probabilities $p_1, \\ldots, p_n$.\n",
    "\n",
    "- we denote such a function as $H(p_1, \\ldots, p_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Requirements for $H$\n",
    "\n",
    "(1) $H(p_1, \\ldots, p_n)$ is a maximum when $\\displaystyle p_1 = \\ldots = p_n = \\frac{1}{n}$\n",
    "\n",
    "(2) $H$ is a symmetric function of the arguments $p_1, \\ldots, p_n$\n",
    "\n",
    "that is, $H(p_1, \\ldots, p_n) = H(p_{\\pi(1)}, \\ldots, p_{\\pi(n)})$ for any permutation $\\pi$ of $(1, \\ldots, n)$\n",
    "\n",
    "$\\Rightarrow$ only the probabilities matter, not their order.\n",
    "\n",
    "(3) $H(p_1, \\ldots, p_n) \\geq 0$ and the equality holds only when one of $p_i = 1$\n",
    "\n",
    "$\\Rightarrow$ uncertainty is inherently a positive quantity, and it is zero only when there is no randomness present\n",
    "      \n",
    "(4) $\\displaystyle H\\left(\\frac{1}{n}, \\ldots, \\frac{1}{n}\\right) \\leq H\\left(\\frac{1}{n + 1}, \\ldots, \\frac{1}{n + 1}\\right)$\n",
    "\n",
    "$\\Rightarrow$ a two-horse race is less uncertain than a three-horse race\n",
    "\n",
    "(5) $H(p_1, \\ldots, p_n, 0) = H(p_1, \\ldots, p_n)$\n",
    "\n",
    "$\\Rightarrow$ uncertainty of a six-sided die is the same as a seven-sided die that has no chance of showing 7 but otherwise fair\n",
    "\n",
    "(6) $H(p_1, \\ldots, p_n)$ is a continuous function of its arguments\n",
    "\n",
    "$\\Rightarrow$ a small change in the probabilities should not drastically affects the uncertainty\n",
    "\n",
    "(7) if $m$ and $n$ are positive integers,\n",
    "$\\displaystyle H\\left(\\frac{1}{mn}, \\ldots, \\frac{1}{mn}\\right) = H\\left(\\frac{1}{m}, \\ldots, \\frac{1}{m}\\right) + H\\left(\\frac{1}{n}, \\ldots, \\frac{1}{n}\\right)$\n",
    "\n",
    "$\\Rightarrow$ uncertainty for throwing an $m$-sided die followed by an $n$-sided die should be the sum of individual uncertainties (linearity condition)\n",
    "\n",
    "(8) let $p = p_1 + \\ldots + p_m$ and $q = q_1 + \\ldots + q_n$ where each $p_i$ and $q_j$ are non-negative;\n",
    "\n",
    "if $p$ and $q$ are positive and $p + q = 1$,\n",
    "$\\displaystyle H(p_1, \\ldots, p_m, q_1, \\ldots, q_n) = H(p, q) + p H\\left(\\frac{p_1}{p}, \\ldots, \\frac{p_m}{p}\\right) + q H\\left(\\frac{q_1}{q}, \\ldots, \\frac{q_n}{q}\\right)$\n",
    "\n",
    "$\\Rightarrow$ think about a race in which there are $m$ brown horses and $n$ grey horses with $p_i$ (or $q_j$) being the probability the $i^{th}$ brown horse (or $j^{th}$ grey horse) wins --- the total uncertainty in the outcome is the uncertainty associated with a brown or grey winner plus the weighted sum of the uncertainties given that the winner is respectively brown or grey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropy\n",
    "\n",
    "**Definition**\n",
    "\n",
    "We calculate the \\textbf{entropy} $H$ by\n",
    "$$\n",
    "  H = \\sum_{i = 1 \\ldots m} p_i \\log_2 \\frac{1}{p_i} = - \\sum_{i = 1 \\ldots m} p_i \\log_2 p_i\n",
    "$$\n",
    "\n",
    "- the log is taken to the **base 2** (ie, entropy is expressed in bits)\n",
    "- the convention is that $\\displaystyle 0\\log_2 \\frac{1}{0} = 0$, because $\\displaystyle \\lim_{a\\rightarrow 0+} a\\log_2 \\frac{1}{a} \\rightarrow 0$\n",
    "- $H$ satisfies the requirements (1) to (8) for measuring the uncertainty\n",
    "- alternatively, the **entropy** of a random variable $X$ is $H(X) = \\sum_{x\\in {\\cal X}} p(x) \\log_2 \\frac{1}{p(x)}$\n",
    "- depending on the context, we may write $H(p)$ for the above quantity\n",
    "\n",
    "**Examples**\n",
    "\n",
    "(1) a flip of a fair coin:\n",
    "\t$$\n",
    "\t  H = \\frac{1}{2}\\log_2 2 + \\frac{1}{2}\\log_2 2 = 1 \\ (\\text{bit})\n",
    "\t$$\n",
    "\n",
    "(2) a flip of a biased coin, with the head coming up as twice frequently as the tail:\n",
    "\t$$\n",
    "\t  H = \\frac{2}{3}\\log_2\\frac{3}{2} + \\frac{1}{3}\\log_2 3 \\sim 0.92 \\ (\\text{bits})\n",
    "\t$$\n",
    "\n",
    "(3) a race between four horses, having a chance to win with probabilities $0.4$, $0.3$, $0.2$, and $0.1$:\n",
    "\t$$\n",
    "\t  H = 0.4\\log_2 \\frac{1}{0.4} + 0.3\\log_2 \\frac{1}{0.3} + 0.2\\log_2 \\frac{1}{0.2} + 0.1\\log_2 \\frac{1}{0.1} \\sim 1.85 \\ (\\text{bits})\n",
    "\t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropy and its Properties\n",
    "\n",
    "Here we use the following *memoryless binary symmetric channel* to calculate various entropies:\n",
    "<img src=\"./figs/binary_channel1.jpg\", width=400, align=center>\n",
    "\n",
    "- (eg) the entropy of information source $X$ in the above channel is\n",
    "  $$\n",
    "    H(X) = - (a\\log_2 a + b\\log_2 b)\n",
    "  $$\n",
    "\n",
    "**Joint entropy**\n",
    "\n",
    "The joint entropy of $X$ and $Y$ is defined by\n",
    "$$\n",
    "    H(X,Y) = \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} p(x,y) \\log_2 \\frac{1}{p(x,y)}\n",
    "$$\n",
    "\n",
    "- (eg) the joint entropy of input $X$ and output $Y$ in the above channel is\n",
    "  \\begin{align*}\n",
    "    H(X,Y) &= - af\\log_2 af - ae\\log_2 ae - be\\log_2 be - bf\\log_2 bf \\\\\n",
    "    &= - (a\\log_2 a + b\\log_2 b) - (e\\log_2 e + f\\log_2 f)\n",
    "  \\end{align*}\n",
    "\n",
    "**Conditional entropy**\n",
    "\n",
    "The conditional entropy of $Y$ given $X$ is defined by\n",
    "\\begin{align*}\n",
    "    H(Y|X) &= \\sum_{x\\in {\\cal X}} p(x) H(Y|X = x) \\\\\n",
    "    &= \\sum_{x\\in {\\cal X}} p(x) \\sum_{y\\in {\\cal Y}} p(y|x) \\log_2 \\frac{1}{p(y|x)} \\\\\n",
    "    &= \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} p(x,y) \\log_2 \\frac{1}{p(y|x)}\n",
    "\\end{align*}\n",
    "\n",
    "- (eg) the conditional entropy of output $Y$ given input $X$ in the above channel is\n",
    "  \\begin{align*}\n",
    "    H(Y|X) &= - af\\log_2 f - ae\\log_2 e - be\\log_2 e - bf\\log_2 f \\\\\n",
    "    &= - (e\\log_2 e + f\\log_2 f)\n",
    "  \\end{align*}\n",
    "\n",
    "**Chain rule**\n",
    "\n",
    "Now we can write the following chain rule:\n",
    "\\begin{align*}\n",
    "    H(X,Y) & = H(X) + H(Y|X)\\\\\n",
    "        & = H(Y) + H(X|Y) \\\\\n",
    "    H(X_1\\ldots,X_n) & = \\sum_{i=1,\\ldots,n} H(X_i|X_1,\\ldots,X_{i-1})\n",
    "\\end{align*}\n",
    "\n",
    "**Mutual information**\n",
    "\n",
    "The mutual information between $X$ and $Y$ is defined by\n",
    "$$\n",
    "    I(X;Y) = \\sum_{x\\in {\\cal X}} \\sum_{y\\in {\\cal Y}} p(x,y) \\log_2 \\frac{p(x,y)}{p(x)p(y)}\n",
    "$$\n",
    "This $I(X;Y)$ can be rewritten as\n",
    "$$\n",
    "    I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\n",
    "$$\n",
    "It measures the average reduction in uncertainty about $X$ that results from learning the value of $Y$.\n",
    "\n",
    "- (eg) mutual information between input $X$ and output $Y$ in the above channel is\n",
    "  $$\n",
    "    I(X;Y) = - (af + be)\\log_2(af + be) - (ae + bf)\\log_2(ae + bf) + (e\\log_2 e + f\\log_2 f)\n",
    "  $$\n",
    "\n",
    "**Binary entropy function**\n",
    "\n",
    "By defining the binary entropy function:\n",
    "$$\n",
    "    H_2(e) \\equiv H(e, 1 - e) \\equiv e\\log_2\\frac{1}{e} + (1 - e)\\log_2\\frac{1}{1 - e}\n",
    "$$\n",
    "entropies for the memoryless binary symmetric channel can be summarised as\n",
    "\\begin{align*}\n",
    "    H(X) &= H_2(a) \\\\\n",
    "    H(Y) &= H_2(a - 2ae + e) \\\\\n",
    "    H(X,Y) &= H_2(a) + H_2(e) \\\\\n",
    "    H(X|Y) &= H_2(a) + H_2(e) - H_2(a - 2ae + e) \\\\\n",
    "    H(Y|X) &= H_2(e) \\\\\n",
    "    I(X;Y) &= H_2(a - 2ae + e) - H_2(e)\n",
    "\\end{align*}\n",
    "The following figure may clarify the relations:\n",
    "<img src=\"./figs/big_picture.jpg\", width=200, align=center>\n",
    "\n",
    "**Relative entropy**\n",
    "\n",
    "The relative entropy between two probability distributions $p(x)$ and $q(x)$, defined over the same random variable $X$, is\n",
    "$$\n",
    "    D(\\mathbf{p}\\ ||\\ \\mathbf{q}) = \\sum_{x\\in {\\cal X}} p(x) \\log_2 \\frac{p(x)}{q(x)}\n",
    "$$\n",
    "It is also referred to as the **Kullback-Leibler divergence**.\n",
    "\n",
    "- It satisfies the **Gibbs inequality**, $D(\\mathbf{p}\\ ||\\ \\mathbf{q}) \\geq 0$, with equality if and only if $p(x) = q(x)$ over $x\\in {\\cal X}$.\n",
    "- It is not a distance because $D(\\mathbf{p}\\ ||\\ \\mathbf{q}) \\neq D(\\mathbf{q}\\ ||\\ \\mathbf{p})$ in general."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
